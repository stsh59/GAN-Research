{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1382dbc-6746-43ba-b603-1aacac24a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf86d721-0f51-43db-afc5-f3b4df5aceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38094dd5-3d4f-47fe-a610-01cc37f91c9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 256 elements not 1024",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 267\u001b[0m\n\u001b[1;32m    264\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), noise_dim, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Generate fake images and segmentations\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m fake_images, fake_segs \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# Train discriminator\u001b[39;00m\n\u001b[1;32m    270\u001b[0m d_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 55\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[0;34m(self, noise, labels)\u001b[0m\n\u001b[1;32m     52\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Generate image and segmentation map separately\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m seg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseg_gen(x)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, seg\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 256 elements not 1024"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, num_classes):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Define the noise and class input dimensions\n",
    "        input_dim = noise_dim + num_classes\n",
    "        \n",
    "        # Generator architecture\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024 * 8 * 8),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        # Transposed convolution layers to generate image and segmentation\n",
    "        self.image_gen = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 1, 4, stride=2, padding=1),\n",
    "            nn.Tanh()  # Image output range: [-1, 1]\n",
    "        )\n",
    "        \n",
    "        self.seg_gen = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 1, 4, stride=2, padding=1),\n",
    "            nn.Sigmoid()  # Segmentation output range: [0, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, noise, labels):\n",
    "        # Concatenate noise and one-hot encoded labels\n",
    "        x = torch.cat((noise, labels), dim=1)\n",
    "        \n",
    "        # Pass through the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 1024, 8, 8)\n",
    "        \n",
    "        # Generate image and segmentation map separately\n",
    "        image = self.image_gen(x)\n",
    "        seg = self.seg_gen(x)\n",
    "        \n",
    "        return image, seg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, device='cuda'):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Define device\n",
    "        self.device = torch.device(device)\n",
    "        \n",
    "        # Network architecture for discriminator\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(2, 64, kernel_size=4, stride=2, padding=1),  # Concatenated input (image + seg)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Flatten()  # Flatten the output to feed into fully connected layer\n",
    "        ).to(self.device)  # Move the main sequential module to the device\n",
    "\n",
    "        # Initialize to None for lazy initialization later\n",
    "        self.fc = None\n",
    "        self.output_layer = None\n",
    "        \n",
    "    def forward(self, image, seg):\n",
    "        # Move inputs to the specified device\n",
    "        image = image.to(self.device)\n",
    "        seg = seg.to(self.device)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Concatenate image and segmentation map\n",
    "        x = torch.cat((image, seg), dim=1)  # Concatenate on the channel dimension\n",
    "        \n",
    "        # Pass through the network\n",
    "        x = self.main(x)\n",
    "        \n",
    "        # Lazy initialization of fully connected layer and output layer\n",
    "        if self.fc is None:\n",
    "            # Initialize the fully connected layer with the correct input size\n",
    "            input_size = x.shape[1]  # Get the number of features in the flattened output\n",
    "            self.fc = nn.Linear(input_size, 1).to(self.device)  # Move to the specified device\n",
    "            self.output_layer = nn.Sigmoid().to(self.device)  # Move to the specified device\n",
    "            self.add_module('fc', self.fc)\n",
    "            self.add_module('output_layer', self.output_layer)\n",
    "        \n",
    "        # Pass through the fully connected and output layers\n",
    "        x = self.fc(x)\n",
    "        output = self.output_layer(x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Custom dataset class\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, data_folder, transform=None):\n",
    "        self.data_folder = data_folder\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Initialize list of image, segmentation, and labels\n",
    "        self.image_files = []\n",
    "        self.seg_files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Define label mapping\n",
    "        self.label_mapping = {\n",
    "            'glioma': 0,\n",
    "            'healthy': 1,\n",
    "            'pituitary': 2,\n",
    "            'meningioma': 3\n",
    "        }\n",
    "        \n",
    "        # Collect file paths and labels\n",
    "        for class_name in self.label_mapping:\n",
    "            class_folder = os.path.join(data_folder, class_name)\n",
    "            for filename in os.listdir(class_folder):\n",
    "                if filename.endswith('.jpg'):\n",
    "                    # Get the corresponding segmentation file\n",
    "                    seg_filename = filename.replace('.jpg', '_mask.png')\n",
    "                    img_path = os.path.join(class_folder, filename)\n",
    "                    seg_path = os.path.join(class_folder, seg_filename)\n",
    "                    \n",
    "                    # Check if segmentation file exists\n",
    "                    if os.path.exists(seg_path):\n",
    "                        self.image_files.append(img_path)\n",
    "                        self.seg_files.append(seg_path)\n",
    "                        self.labels.append(self.label_mapping[class_name])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Load image, segmentation, and label\n",
    "        img_path = self.image_files[index]\n",
    "        seg_path = self.seg_files[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        # Load image and segmentation map\n",
    "        img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "        seg = Image.open(seg_path).convert('L')  # Convert to grayscale\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            seg = self.transform(seg)\n",
    "        \n",
    "        # One-hot encode the label\n",
    "        label_one_hot = torch.zeros(4)\n",
    "        label_one_hot[label] = 1\n",
    "        \n",
    "        return img, seg, label_one_hot\n",
    "\n",
    "# Hyperparameters\n",
    "noise_dim = 100\n",
    "num_classes = 4  # Number of different classes\n",
    "output_channels = 1  # Grayscale images\n",
    "\n",
    "# Define the device for computation (GPU/CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Instantiate the generator and discriminator\n",
    "generator = Generator(noise_dim, num_classes).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Optimizers\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Load the dataset and create data loader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Set the path to your data folder\n",
    "data_folder = \"./tumor_dataset\"\n",
    "dataset = SegmentationDataset(data_folder, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to visualize four sample generated images for each class\n",
    "def visualize_fake_images_and_segs(fake_images, fake_segs, labels, epoch, step):\n",
    "    # Convert the generated images and segmentations to numpy arrays and denormalize\n",
    "    fake_images_np = (fake_images.detach().cpu().numpy() * 0.5 + 0.5).squeeze(1)  # Denormalize and squeeze channels\n",
    "    fake_segs_np = fake_segs.detach().cpu().numpy().squeeze(1)  # Convert segmentation to numpy and squeeze channels\n",
    "    labels_np = labels.detach().cpu().numpy()  # Convert labels to numpy arrays\n",
    "\n",
    "    # Define the number of classes\n",
    "    num_classes = labels_np.shape[1]\n",
    "\n",
    "    # Create a figure with a grid layout (4 rows for each class, 2 columns for image and segmentation)\n",
    "    fig, axes = plt.subplots(num_classes, 2, figsize=(12, 12))\n",
    "    \n",
    "    # Iterate through each class\n",
    "    for class_idx in range(num_classes):\n",
    "        # Find the indices of images with the current class label\n",
    "        class_indices = np.where(labels_np[:, class_idx] == 1)[0]\n",
    "        \n",
    "        # Sample one image and segmentation for the current class\n",
    "        sampled_idx = np.random.choice(class_indices)\n",
    "        \n",
    "        # Plot the image\n",
    "        axes[class_idx, 0].imshow(fake_images_np[sampled_idx], cmap='gray')\n",
    "        axes[class_idx, 0].set_title(f\"Class {class_idx + 1} Image\")\n",
    "        axes[class_idx, 0].axis('off')\n",
    "        \n",
    "        # Plot the segmentation\n",
    "        axes[class_idx, 1].imshow(fake_segs_np[sampled_idx], cmap='gray')\n",
    "        axes[class_idx, 1].set_title(f\"Class {class_idx + 1} Segmentation\")\n",
    "        axes[class_idx, 1].axis('off')\n",
    "    \n",
    "    # Set the overall title for the figure\n",
    "    plt.suptitle(f\"Epoch {epoch + 1}, Step {step}\")\n",
    "    plt.show()\n",
    "\n",
    "# Training the GAN\n",
    "num_epochs = 50000\n",
    "visualize_interval = 1000  # Adjust interval as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    for step, (images, segs, labels) in enumerate(dataloader):\n",
    "        # Move data to the device\n",
    "        images = images.to(device)\n",
    "        segs = segs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Generate noise\n",
    "        noise = torch.randn(images.size(0), noise_dim, device=device)\n",
    "        \n",
    "        # Generate fake images and segmentations\n",
    "        fake_images, fake_segs = generator(noise, labels)\n",
    "        \n",
    "        # Train discriminator\n",
    "        d_optimizer.zero_grad()\n",
    "        \n",
    "        # Real data labels\n",
    "        real_labels = torch.ones(images.size(0), device=device)\n",
    "        fake_labels = torch.zeros(images.size(0), device=device)\n",
    "        \n",
    "        # Real data output\n",
    "        real_output = discriminator(images, segs)\n",
    "        d_loss_real = criterion(real_output.squeeze(), real_labels)\n",
    "        \n",
    "        # Fake data output\n",
    "        fake_output = discriminator(fake_images, fake_segs)\n",
    "        d_loss_fake = criterion(fake_output.squeeze(), fake_labels)\n",
    "        \n",
    "        # Total discriminator loss\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # Train generator\n",
    "        g_optimizer.zero_grad()\n",
    "        \n",
    "        # Pass the generated fake data through the discriminator again\n",
    "        fake_output = discriminator(fake_images, fake_segs)\n",
    "        \n",
    "        # Generator loss\n",
    "        g_loss = criterion(fake_output.squeeze(), real_labels)\n",
    "        \n",
    "        g_loss.backward(retain_graph=True)\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        # Print losses occasionally\n",
    "        if epoch % 100 == 0 and step==0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{step}/{len(dataloader)}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "        \n",
    "        # Visualize four sample generated images for each class at the specified interval\n",
    "        if (epoch % visualize_interval==0 or epoch ==0)  and step == 0:\n",
    "            visualize_fake_images_and_segs(fake_images, fake_segs, labels, epoch, step)\n",
    "    \n",
    "    # Save the model checkpoints if necessary\n",
    "    # torch.save(generator.state_dict(), \"generator.pth\")\n",
    "    # torch.save(discriminator.state_dict(), \"discriminator.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892064f-48d4-4a61-834c-515ba8da838b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd48c1-f3ab-4423-9c9c-6064ee5cf386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
